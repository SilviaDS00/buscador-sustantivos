# -*- coding: utf-8 -*-
"""LecturadeFicheros.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pxANxmxU3rtkvRMAjwQSd-Own7NeN7xM

Para subir un archivo desde local:  

from google.colab import files  

files.upload()

se nos abre un cuadro para subir los archivos.
El directorio raiz de colab es /content
Si subimos un archivo, por ejemplo, ejemplo.csv. El path sería: /content/ejemplo.sv

para descargar el archivo de la máquina remota: fieles.download("ejemplo.csv")

Si tenemos los archivos en drive, y montamos drive en la máquina ,podemos acceder a dichos archivos como si estuvieran en local. Para ello:  

from google.colab import drive  

drive.mount('/content/drive')

Nuestro drive se ha montado en /content/drive.
Nos abre una url, que nos pide un código para autorizar a Colaba utilizar nuestro drive y lo tenemos que pegar en el input uqe aparece.

# Lectura de ficheros de texto plano locales

Subimos los ficheros
"""

from google.colab import files

files.upload()

import os
# /content es la carperta raiz de Colab
path = '/content'
ficheros = os.listdir(path)

documentos = []


for nombrefichero in ficheros:    
        if os.path.isfile(os.path.join(path, nombrefichero)): #si es un fichero y no es un directorio
            fich = open(os.path.join(path, nombrefichero), "r")
            text = fich.read() #leemos el contenido y lo añadimos a documentos
            documentos.append(text)
            
print(documentos)

"""# Lectura de páginas web"""

#librería para parsear y limpiar las páginas web (etiquetas, javascript)
!pip install beautifulsoup4
from bs4 import BeautifulSoup

#librería urllib para leer páginas web
import urllib.request

#La instrucción with abre la página y cuando termina la cierra.
#llamamos al método urlopen, pasandole la ruta (páginaweb). Dejando la respuesta en response.
#response trendrá el apunte a la web. Entonces lo leemos y decocificamos con 
# response.read.decode()

#paginaweb='https://agenciacolocacion.ayto-albacete.es'
with urllib.request.urlopen(paginaweb) as response:
   html = response.read().decode() #cuidado si está bien codificada o no la web

#Le decimos que me limpie las etiquetas html
soup = BeautifulSoup(html, features="html.parser")

#Esto mismo lo podemos hacer con el módulo re. Sutituyendo el patrón por nada.
#import re
#print(re.sub('<[^<]+?>', '', html))

# Borra elementos de script y style 
for script in soup(["script", "style"]):
    script.extract()    # borrar

#Con el método get_text recuperamos el texto completo.
print(soup.get_text())

#Una vez obtenido el texto, habría que preprocesar el texto, pasarle las stopword, etc.

"""# Lectura de archivos CSV  
Son archivos de excel, son textos planos con separadores que suelen ser coma o a veces ;. Habrá que tener en cuenta si hay una cabecera.
"""

import pandas as pd
#se puede usar ruta local. Por Ej: r"C:\\Users\\piyush\\Downloads\\\Iris.csv"
#O lo podemos tener online
#Lo leemos con el método read_csv y nos devuelve un objeto llamado 
#data frame
#Con header=None le indicamos que tenga encuenta la cabecera
df = pd.read_csv("https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data",  header=None) 
#Con el método head podemos ver el contenido
df.head()

#Si queremos leer un dato en concreto hay diversos mecanios.
df = pd.read_csv("https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data",  header=0, sep=',') 
#df = pd.read_csv("C:\\Users\\elmus\\Documents\\prueba2\\Train.csv", header=0, sep=',') 
#quita la cabecera, e indica que el separador es la coma.

#le indicamos la columna con el índice de localización y lo convierte a una lista, dejándolo en vector
vector = df.iloc[1:,4].values.tolist()
print(vector)